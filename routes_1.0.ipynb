{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de rutas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerias necesarias\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from geopy.distance import great_circle\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import linkage ,fcluster\n",
    "import googlemaps\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\martin.olivares\\Desktop\\projects\\best-route\\data\\adt.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de valores erroneos:\n",
      "address             0\n",
      "hora_recogida       0\n",
      "destino             0\n",
      "nombre_pasajero     0\n",
      "mail                0\n",
      "phone               0\n",
      "fecha               0\n",
      "num_empty_cells     0\n",
      "nulls              47\n",
      "datetime_1          0\n",
      "dtype: int64\n",
      "-----------------------------------\n",
      "Lista errores:\n"
     ]
    }
   ],
   "source": [
    "#limpiar data\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['address']=data['Direccion de inicio']\n",
    "df['hora_recogida']=data['Hora de recogida']\n",
    "df['destino']=data['Dirección destino']\n",
    "df['nombre_pasajero']=data['Nombre de pasajero']                     #nueva\n",
    "df['mail']=data['Correo pasajero']                                  #nueva\n",
    "df['phone']=data['Telefono de contacto']#.apply(lambda x: round(x))\n",
    "df['fecha']=data['fecha']\n",
    "\n",
    "df[\"num_empty_cells\"] = df.isna().sum(axis=1)\n",
    "df[\"nulls\"]=df['num_empty_cells']/max(df['num_empty_cells'])\n",
    "\n",
    "\n",
    "# Función para crear objetos datetime\n",
    "def crear_datetime(row):\n",
    "    fecha_str = row['fecha']\n",
    "    hora_str = row['hora_recogida']\n",
    "    # Convertir la cadena de texto de hora a un objeto time\n",
    "    hora = datetime.strptime(hora_str, '%H:%M').time()\n",
    "    # Crear un objeto datetime a partir de la fecha y hora\n",
    "    fecha = datetime.strptime(fecha_str, '%Y-%m-%d').date()\n",
    "    fecha_y_hora = datetime.combine(fecha, hora)\n",
    "    return fecha_y_hora\n",
    "\n",
    "# Agregar una nueva columna datetime y conservar las columnas existentes\n",
    "df = df.assign(datetime_1=df.apply(crear_datetime, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "print('Numero de valores erroneos:')\n",
    "print(df.isna().sum())\n",
    "print('-----------------------------------')\n",
    "print('Lista errores:')\n",
    "#df[df.isna().any(axis=1)]\n",
    "\n",
    "df = df.drop(df[df['nulls']==1].index)\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "df.drop(columns=['nulls','num_empty_cells'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corregir typos\n",
    "# Definir un diccionario con las abreviaturas de calles y sus correspondientes formas completas\n",
    "street_abbreviations = {\n",
    "    \"cl\": \"calle\",\n",
    "    \"av\": \"avenida\",\n",
    "    \"pj\": \"pasaje\",\n",
    "    \"cam\": \"camino\",\n",
    "    \"nte\": \"norte\",\n",
    "    \"hermnos\":'hnos',\n",
    "    'hmnos':'hermanos',\n",
    "    'tte':'teniente',\n",
    "    'concon':'con con'\n",
    "    }\n",
    "\n",
    "\n",
    "# Definir una función que corrija las abreviaturas de calles en una dirección\n",
    "def correct_typos(address):\n",
    "    for abbreviation, full_form in street_abbreviations.items():\n",
    "        address = re.sub(r'\\b{}\\b'.format(abbreviation), full_form, address)\n",
    "    return address\n",
    "\n",
    "# Aplicar la función a cada dirección del DataFrame\n",
    "df[\"address\"] = df[\"address\"].str.lower().apply(correct_typos)\n",
    "df['destino'] = df[\"destino\"].str.lower().apply(correct_typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear funciones de caché y de limpieza de direcciones\n",
    "\n",
    "clave_api='AIzaSyAvTzCycvOetN-NA51GNqxb80d-Ma-0Azg'\n",
    "gmaps = googlemaps.Client(key=clave_api)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################# corregir typos\n",
    "\n",
    "googlemaps_cache = 'googlemaps_cache.pkl'\n",
    "\n",
    "def load_cache():\n",
    "    # Cargar la caché desde el archivo si existe\n",
    "    if os.path.exists(googlemaps_cache):\n",
    "        with open(googlemaps_cache, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    # Guardar la caché en el archivo\n",
    "    with open(googlemaps_cache, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "def correct_address(direccion):\n",
    "    # Cargar la caché desde el archivo\n",
    "    cache = load_cache()\n",
    "    # Verificar si el resultado de la dirección ya está en la caché\n",
    "    if direccion in cache:\n",
    "        return cache[direccion]\n",
    "    # Si el resultado de la dirección no está en la caché, realizar una solicitud a la API de Google Maps\n",
    "    geocode_result = gmaps.geocode(direccion) \n",
    "    if len(geocode_result) > 0:\n",
    "        formatted_address = geocode_result[0]['formatted_address']\n",
    "        # Agregar el resultado a la caché para futuras solicitudes\n",
    "        cache[direccion] = formatted_address\n",
    "        save_cache(cache)\n",
    "        return formatted_address\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "################################################################# reverse geocoding\n",
    "\n",
    "googlemaps_reverse_cache = 'googlemaps_reverse_cache.pkl'\n",
    "\n",
    "def load_reverse_cache():\n",
    "    # Cargar la caché desde el archivo si existe\n",
    "    if os.path.exists(googlemaps_reverse_cache):\n",
    "        with open(googlemaps_reverse_cache, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def save_reverse_cache(cache):\n",
    "    # Guardar la caché en el archivo\n",
    "    with open(googlemaps_reverse_cache, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "def reverse_geocode(lat, lon):\n",
    "    # Cargar la caché desde el archivo\n",
    "    cache = load_reverse_cache()\n",
    "    # Verificar si el resultado de la dirección ya está en la caché\n",
    "    if (lat, lon) in cache:\n",
    "        return cache[(lat, lon)]\n",
    "    # Si el resultado de la dirección no está en la caché, realizar una solicitud a la API de Google Maps para hacer reverse geocoding\n",
    "    reverse_geocode_result = gmaps.reverse_geocode((lat, lon))      \n",
    "    if len(reverse_geocode_result) > 0:\n",
    "        formatted_address = reverse_geocode_result[0]['formatted_address']\n",
    "        # Agregar el resultado a la caché para futuras solicitudes\n",
    "        cache[(lat, lon)] = formatted_address\n",
    "        save_reverse_cache(cache)\n",
    "        return formatted_address\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "############################################################### georeferenciación\n",
    "\n",
    "# Comprobamos si el archivo existe y no está vacío antes de cargar la memoria caché\n",
    "if os.path.exists(\"coordinates_googlemaps.pkl\") and os.path.getsize(\"geocode_cache.pickle\") > 0:\n",
    "    with open(\"coordinates_googlemaps.pkl\", \"rb\") as f:\n",
    "        geocode_cache = pickle.load(f)\n",
    "else:\n",
    "    geocode_cache = {}\n",
    "\n",
    "# Creamos una función para guardar la memoria caché en un archivo externo\n",
    "def save_geocode_cache():\n",
    "    with open(\"coordinates_googlemaps.pkl\", \"wb\") as f:\n",
    "        pickle.dump(geocode_cache, f)\n",
    "\n",
    "# Creamos una función para geolocalizar una dirección y almacenar las coordenadas en la caché\n",
    "def geolocate(address):\n",
    "    if address in geocode_cache:\n",
    "        return geocode_cache[address]\n",
    "    else:\n",
    "        geocode_result = gmaps.geocode(address)    \n",
    "        if len(geocode_result) > 0:\n",
    "            location = geocode_result[0]['geometry']['location']\n",
    "            coordinates = (location['lat'], location['lng'])\n",
    "            geocode_cache[address] = coordinates\n",
    "            save_geocode_cache()  # Guardamos la memoria caché en un archivo externo\n",
    "            return coordinates\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "\n",
    "############################################################## georeferenciación 2\n",
    "\n",
    "\n",
    "\n",
    "# Establecer el nombre del archivo de caché\n",
    "googlemaps_geocode_cache = 'googlemaps_geocode_cache.pkl'\n",
    "\n",
    "def load_geocode_cache():\n",
    "    # Cargar la caché desde el archivo si existe\n",
    "    if os.path.exists(googlemaps_geocode_cache):\n",
    "        with open(googlemaps_geocode_cache, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def save_geocode_cache(cache):\n",
    "    # Guardar la caché en el archivo\n",
    "    with open(googlemaps_geocode_cache, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "def geocode(direccion):\n",
    "    # Cargar la caché desde el archivo\n",
    "    cache = load_geocode_cache()\n",
    "    # Verificar si el resultado de la geocodificación ya está en la caché\n",
    "    if direccion in cache:\n",
    "        return cache[direccion]\n",
    "    # Si el resultado de la geocodificación no está en la caché, realizar una solicitud a la API de Google Maps para hacer geocodificación\n",
    "    geocode_result = gmaps.geocode(direccion)      \n",
    "    if len(geocode_result) > 0:\n",
    "        lat = geocode_result[0]['geometry']['location']['lat']\n",
    "        lng = geocode_result[0]['geometry']['location']['lng']\n",
    "        # Agregar el resultado a la caché para futuras solicitudes\n",
    "        cache[direccion] = (lat, lng)\n",
    "        save_geocode_cache(cache)\n",
    "        return (lat, lng)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "############################################################## distance matrix\n",
    "\n",
    "\n",
    "############################################################## directions\n",
    "\n",
    "\n",
    "# Establecer el nombre del archivo de caché\n",
    "googlemaps_directions_cache = 'googlemaps_directions_cache.pkl'\n",
    "\n",
    "def load_directions_cache():\n",
    "    # Cargar la caché desde el archivo si existe\n",
    "    if os.path.exists(googlemaps_directions_cache):\n",
    "        with open(googlemaps_directions_cache, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def save_directions_cache(cache):\n",
    "    # Guardar la caché en el archivo\n",
    "    with open(googlemaps_directions_cache, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "def directions(origin, destination, waypoints, mode='driving',departure_time=datetime.now()):    \n",
    "    # Crear una clave única de caché para esta solicitud de dirección\n",
    "    cache_key = f\"{origin}_{destination}_{waypoints}_{mode}\"\n",
    "    \n",
    "    # Cargar la caché desde el archivo\n",
    "    cache = load_directions_cache()\n",
    "    \n",
    "    # Verificar si el resultado de la dirección ya está en la caché\n",
    "    if cache_key in cache:\n",
    "        return cache[cache_key]\n",
    "    \n",
    "    # Si el resultado de la dirección no está en la caché, realizar una solicitud a la API de Google Maps para obtener la dirección\n",
    "    result = gmaps.directions(\n",
    "        origin=origin,\n",
    "        destination=destination,\n",
    "        waypoints=waypoints,\n",
    "        mode=mode,\n",
    "        departure_time= departure_time)\n",
    "    \n",
    "    if len(result) > 0:\n",
    "        # Agregar el resultado a la caché para futuras solicitudes\n",
    "        cache[cache_key] = result\n",
    "        save_directions_cache(cache)\n",
    "        return result\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corregir direcciones y saber si hay valores erroneos\n",
    "\n",
    "df['address'] = df['address'].apply(correct_address)\n",
    "df['destino'] = df['destino'].apply(correct_address)\n",
    "\n",
    "df['aux'] = ~(df.groupby(['destino', 'datetime_1'])['address'].transform('nunique') > 1)\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separar por muchos origenes un destino, muchos destinos un origen y un origen un destino\n",
    "df.sort_values(by=['datetime_1'],inplace=True)\n",
    "df['id'] = range(len(df))\n",
    "\n",
    "false_df = df[df['aux']==False]\n",
    "true_df = df[df['aux']==True]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(len(true_df))\n",
    "\n",
    "#ahora se obtiene cuando existen un origen un destino\n",
    "# 1. marcar con un flag y sacarlas del calculo de distancias\n",
    "# 2. contar la cantidad de personas y definir el producto\n",
    "\n",
    "#true_df['aux1'] = true_df.duplicated(subset=['hora_recogida', 'address','destino','fecha'], keep=False)\n",
    "#same_origin=true_df[true_df['aux1']==True]\n",
    "#true_df=true_df[true_df['aux1']==False]\n",
    "\n",
    "#print(len(true_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 1.1.1 (distintos origenes -> un destino; crear clusters)\n",
    "\n",
    "false_df['label'] = np.nan\n",
    "\n",
    "# Agrega una nueva columna con la fecha y hora de recogida combinadas\n",
    "false_df['datetime'] = false_df['fecha'] + ' ' + false_df['hora_recogida']\n",
    "\n",
    "# Agrupa por fecha y hora de recogida combinadas, y por hora de recogida\n",
    "datetime_grouped = false_df.groupby('datetime')\n",
    "\n",
    "# Recorre cada grupo y aplica clustering\n",
    "for name, group in datetime_grouped:\n",
    "    # Calcula la distancia entre cada par de direcciones\n",
    "    X = np.zeros((len(group), len(group)))\n",
    "    for i in range(len(group)):\n",
    "        address1 = group.iloc[i]['address']\n",
    "        loc1 = geolocate(address1) # Utilizamos nuestra función personalizada para geolocalizar la dirección\n",
    "        if loc1 is not None:\n",
    "            lat1, lon1 = loc1\n",
    "            point1 = (lat1, lon1)\n",
    "            for j in range(i+1, len(group)):\n",
    "                address2 = group.iloc[j]['address']\n",
    "                loc2 = geolocate(address2) # Utilizamos nuestra función personalizada para geolocalizar la dirección\n",
    "                if loc2 is not None:\n",
    "                    lat2, lon2 = loc2\n",
    "                    point2 = (lat2, lon2)\n",
    "                    X[i, j] = great_circle(point1, point2).m\n",
    "                    X[j, i] = great_circle(point1, point2).m\n",
    "                    \n",
    "    # Crea una matriz con las distancias\n",
    "        kmeans = KMeans(n_clusters=1)\n",
    "        kmeans.fit(X)\n",
    "        group['label'] = kmeans.labels_\n",
    "        labels = np.zeros(len(group))\n",
    "        \n",
    "        # Asigna un número de cluster único a cada grupo\n",
    "        last_label = false_df['label'].max()\n",
    "        if np.isnan(last_label):\n",
    "            cluster_label = 1\n",
    "        else:\n",
    "            cluster_label = last_label + 1\n",
    "        for i in range(len(group)):\n",
    "            if labels[i] == 0:\n",
    "                labels[i] = cluster_label\n",
    "                for j in range(i+1, len(group)):\n",
    "                    if labels[j] == 0 and np.sum(labels == cluster_label) < 8 and X[i, j] > 0:\n",
    "                        labels[j] = cluster_label\n",
    "                if np.sum(labels == cluster_label) > 0:\n",
    "                    cluster_label += 1\n",
    "    \n",
    "    # Asigna las etiquetas de los clusters al dataframe original\n",
    "    group['label'] = labels\n",
    "    false_df.loc[group.index, \"label\"] = group[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 1.1.2 (distintos origenes -> un destino; agregar destino de la ruta)\n",
    "\n",
    "false_df=false_df[['hora_recogida','address','destino','label','id','nombre_pasajero','mail','phone','datetime_1']] #######\n",
    "false_df=false_df.sort_values(by=['label'])\n",
    "\n",
    "grouped= false_df.groupby(['destino','label'])\n",
    "\n",
    "for data,group in grouped:\n",
    "    group.reset_index(drop=True,inplace=True)\n",
    "    nueva_fila = {'hora_recogida': group['hora_recogida'][0], 'address': data[0], 'destino': data[0],'label': data[1]}\n",
    "    false_df = false_df.append(nueva_fila, ignore_index=True)\n",
    "    \n",
    "false_df['aux']= false_df['address']==false_df['destino']\n",
    "\n",
    "false_df=false_df.sort_values(by=['label']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "REQUEST_DENIED (This API project is not authorized to use this API.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m ubicaciones_restantes \u001b[39m=\u001b[39m [(i, id_) \u001b[39mfor\u001b[39;00m i, id_ \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(ubicaciones)), data[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [idx \u001b[39mfor\u001b[39;00m idx, _ \u001b[39min\u001b[39;00m ruta_optima]]\n\u001b[0;32m     37\u001b[0m \u001b[39m# Calcular la distancia de la ubicación actual a cada ubicación restante\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m#************************************************************************************************************************************************\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m distances \u001b[39m=\u001b[39m gmaps\u001b[39m.\u001b[39;49mdistance_matrix(ubicacion_actual, [ubicaciones[i] \u001b[39mfor\u001b[39;49;00m i, _ \u001b[39min\u001b[39;49;00m ubicaciones_restantes], mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdriving\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     40\u001b[0m distances \u001b[39m=\u001b[39m distances[\u001b[39m'\u001b[39m\u001b[39mrows\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39melements\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     41\u001b[0m \u001b[39m# Ordenar las ubicaciones restantes por distancia al siguiente destino\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\martin.olivares\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\googlemaps\\client.py:445\u001b[0m, in \u001b[0;36mmake_api_method.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    444\u001b[0m     args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_extra_params \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mextra_params\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 445\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    446\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[39mdel\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_extra_params\n",
      "File \u001b[1;32mc:\\Users\\martin.olivares\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\googlemaps\\distance_matrix.py:139\u001b[0m, in \u001b[0;36mdistance_matrix\u001b[1;34m(client, origins, destinations, mode, language, avoid, units, departure_time, arrival_time, transit_mode, transit_routing_preference, traffic_model, region)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m region:\n\u001b[0;32m    137\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mregion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m region\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m client\u001b[39m.\u001b[39;49m_request(\u001b[39m\"\u001b[39;49m\u001b[39m/maps/api/distancematrix/json\u001b[39;49m\u001b[39m\"\u001b[39;49m, params)\n",
      "File \u001b[1;32mc:\\Users\\martin.olivares\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\googlemaps\\client.py:340\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, url, params, first_request_time, retry_counter, base_url, accepts_clientid, extract_body, requests_kwargs, post_json)\u001b[0m\n\u001b[0;32m    338\u001b[0m     result \u001b[39m=\u001b[39m extract_body(response)\n\u001b[0;32m    339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_body(response)\n\u001b[0;32m    341\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msent_times\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\martin.olivares\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\googlemaps\\client.py:369\u001b[0m, in \u001b[0;36mClient._get_body\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m api_status \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOVER_QUERY_LIMIT\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    366\u001b[0m     \u001b[39mraise\u001b[39;00m googlemaps\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39m_OverQueryLimit(\n\u001b[0;32m    367\u001b[0m         api_status, body\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merror_message\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m--> 369\u001b[0m \u001b[39mraise\u001b[39;00m googlemaps\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mApiError(api_status,\n\u001b[0;32m    370\u001b[0m                                      body\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merror_message\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;31mApiError\u001b[0m: REQUEST_DENIED (This API project is not authorized to use this API.)"
     ]
    }
   ],
   "source": [
    "#paso 1.1.3 (distintos origenes -> un destino; calcular la ruta más eficiente)\n",
    "\n",
    "# Agregar columnas para latitud y longitud\n",
    "false_df['latitud'] = None\n",
    "false_df['longitud'] = None\n",
    "\n",
    "# Iterar por cada dirección y obtener las coordenadas\n",
    "for index, row in false_df.iterrows():\n",
    "    direccion = row['address']\n",
    "    lat,lng = geocode(direccion) \n",
    "    false_df.at[index, 'latitud'] = lat\n",
    "    false_df.at[index, 'longitud'] = lng\n",
    "    \n",
    "# Crear una lista vacía para almacenar los dataframes con los órdenes de las ubicaciones\n",
    "orden_dfs = []\n",
    "##------------------------------------------------------------------------- bien hasta acá\n",
    "# Agrupar el dataframe por cluster\n",
    "grupos = false_df.groupby('label')\n",
    "\n",
    "# Iterar por cada cluster\n",
    "\n",
    "for grupo, data in grupos:\n",
    "    data = data.reset_index(drop=True)\n",
    "    ubicaciones = list(zip(data['latitud'], data['longitud']))\n",
    "    \n",
    "    # Definir el primer destino como el origen\n",
    "    primer_destino_idx = data.loc[data['aux'] == True].iloc[-1].name\n",
    "    primer_destino_id = data.loc[data['aux'] == True, 'id'].iloc[-1]\n",
    "    ruta_optima = [(primer_destino_idx, primer_destino_id)]\n",
    "    \n",
    "    # Iterar hasta que se hayan agregado todos los destinos a la ruta\n",
    "    while len(ruta_optima) < len(data):\n",
    "        # Obtener la ubicación actual y las ubicaciones restantes\n",
    "        ubicacion_actual_idx, ubicacion_actual_id = ruta_optima[-1]\n",
    "        ubicacion_actual = ubicaciones[ubicacion_actual_idx]\n",
    "        ubicaciones_restantes = [(i, id_) for i, id_ in zip(range(len(ubicaciones)), data['id']) if i not in [idx for idx, _ in ruta_optima]]\n",
    "        # Calcular la distancia de la ubicación actual a cada ubicación restante\n",
    "        #************************************************************************************************************************************************\n",
    "        distances = gmaps.distance_matrix(ubicacion_actual, [ubicaciones[i] for i, _ in ubicaciones_restantes], mode='driving')\n",
    "        distances = distances['rows'][0]['elements']\n",
    "        # Ordenar las ubicaciones restantes por distancia al siguiente destino\n",
    "        sorted_indices = sorted(range(len(distances)), key=lambda k: distances[k]['distance']['value'])\n",
    "        # Elegir el siguiente destino como la ubicación más cercana\n",
    "        siguiente_destino_idx, siguiente_destino_id = ubicaciones_restantes[sorted_indices[0]]\n",
    "        # Agregar el siguiente destino y su id a la ruta óptima\n",
    "        ruta_optima.append((siguiente_destino_idx, siguiente_destino_id))\n",
    "        \n",
    "    # Revertir el orden de los elementos en la lista ruta_optima\n",
    "    ruta_optima = ruta_optima[::-1]\n",
    "        \n",
    "    # Crear un DataFrame y añadir el orden y el id de data\n",
    "    orden_ubicaciones = [(ubicaciones[i], id_) for i, id_ in ruta_optima]\n",
    "    # Divide cada tupla en dos columnas separadas\n",
    "    coordenadas=[tupla[0] for tupla in orden_ubicaciones]\n",
    "    ids=[tupla[1] for tupla in orden_ubicaciones]\n",
    "    df_ids = pd.DataFrame(ids, columns=['id'])\n",
    "    orden_df = pd.DataFrame(coordenadas, columns=['lat', 'lon'])\n",
    "    orden_df = pd.concat([orden_df, df_ids], axis=1)\n",
    "    orden_df['orden'] = range(len(orden_df))  \n",
    "    orden_df['label'] = grupo\n",
    "    # Agregar el dataframe con el orden de las ubicaciones a la lista de dataframes\n",
    "    orden_dfs.append(orden_df)\n",
    "\n",
    "if len(orden_dfs)!=0:\n",
    "    #Concatenar los dataframes en un solo dataframe\n",
    "    false_orden_dfs = pd.concat(orden_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1108\n",
      "1031\n",
      "1064\n",
      "663\n",
      "225\n",
      "1064\n",
      "663\n",
      "225\n",
      "1108\n",
      "1031\n",
      "663\n",
      "225\n",
      "1049\n",
      "934\n",
      "1029\n",
      "1031\n",
      "965\n",
      "936\n",
      "431\n",
      "1698\n",
      "689\n"
     ]
    }
   ],
   "source": [
    "#paso 1.1.4 (distintos origenes -> un destino; obtener tiempo de ruta)\n",
    "\n",
    "if 'false_orden_dfs' in globals():\n",
    "    duraciones = []\n",
    "    last_label = 0  # Variable para llevar la cuenta del último label utilizado\n",
    "    for data, group in false_orden_dfs.groupby('label'):\n",
    "        durations = []\n",
    "        durations.append(None)\n",
    "        for i in range(len(group)-1):\n",
    "            waypoints = group.iloc[i+1:].values.tolist() + [group.iloc[-1]]\n",
    "            route = directions(origin=group.iloc[i], \n",
    "                                    destination=group.iloc[-1],\n",
    "                                    waypoints=waypoints)\n",
    "            duration = route[0]['legs'][0]['duration']['value']\n",
    "            print(duration)\n",
    "            durations.append(duration)\n",
    "        # Agregar la lista de las durations de cada recorrido a la lista duraciones\n",
    "        duraciones += durations\n",
    "        \n",
    "    duraciones=pd.DataFrame(duraciones,columns=['tiempo'])\n",
    "    duraciones['tiempo'] = duraciones['tiempo'].apply(lambda x: round(x/60,1) if not pd.isna(x) else x)\n",
    "    \n",
    "    false_orden_dfs=pd.concat([false_orden_dfs,duraciones],axis=1)\n",
    "    \n",
    "    # rellenar los valores NaN con ceros\n",
    "    false_orden_dfs['tiempo'] = false_orden_dfs['tiempo'].fillna(0)\n",
    "    \n",
    "    # agregar una nueva columna con la suma de tiempo por cluster\n",
    "    false_orden_dfs['tiempo_total'] = false_orden_dfs.groupby('label')['tiempo'].transform('sum')\n",
    "    \n",
    "    # Dividir el grupo si el tiempo total es mayor a 90\n",
    "    if false_orden_dfs['tiempo_total'].max() > 90:\n",
    "        group_size = len(group)\n",
    "        num_subgroups = 2\n",
    "        \n",
    "        # Calcular el número de elementos por subgrupo\n",
    "        elements_per_subgroup = group_size // num_subgroups\n",
    "        remainder = group_size % num_subgroups\n",
    "        \n",
    "        # Verificar si es necesario ajustar el tamaño de los subgrupos\n",
    "        if remainder == 0:\n",
    "            subgroups = np.array_split(group, num_subgroups)\n",
    "        else:\n",
    "            subgroups = []\n",
    "            start = 0\n",
    "            for i in range(num_subgroups):\n",
    "                end = start + elements_per_subgroup + (1 if i < remainder else 0)\n",
    "                # si i es menor que el resto, agrega 1 al tamaño del subgrupo actual\n",
    "                subgroup = group[start:end] \n",
    "                subgroups.append(subgroup)\n",
    "                start = end \n",
    "        \n",
    "        \n",
    "        # Calcular las duraciones de los subgrupos\n",
    "        duraciones = []\n",
    "        for g in subgroups:\n",
    "            durations = []\n",
    "            durations.append(None)\n",
    "            for i in range(len(g)-1):\n",
    "                waypoints = g.iloc[i+1:].values.tolist() + [g.iloc[-1]]\n",
    "                route = directions(origin=g.iloc[i],  \n",
    "                                        destination=g.iloc[-1],\n",
    "                                        waypoints=waypoints)\n",
    "                duration = route[0]['legs'][0]['duration']['value']\n",
    "                durations.append(duration)\n",
    "            # Agregar la lista de las durations de cada recorrido a la lista duraciones\n",
    "            duraciones += durations\n",
    "        duraciones=pd.DataFrame(duraciones,columns=['tiempo'])\n",
    "        duraciones['tiempo']=duraciones['tiempo'].apply(lambda x: round(x/60,1))\n",
    "        # Concatenar los resultados para cada subgrupo\n",
    "        subgroups_dataframes = []\n",
    "        last_label = 0\n",
    "        for i, subgroup in enumerate(subgroups):\n",
    "            # Incrementar el label para el nuevo subgrupo\n",
    "            last_label += 1\n",
    "            subgroup_dataframe = pd.DataFrame(subgroup).assign(label=last_label,tiempo=duraciones[i:]).reset_index(drop=True)\n",
    "            subgroups_dataframes.append(subgroup_dataframe)\n",
    "        \n",
    "        false_orden_dfs = pd.concat(subgroups_dataframes)\n",
    "        # Reiniciar la columna \"orden\" para cada nuevo label creado\n",
    "        false_orden_dfs['orden'] = false_orden_dfs.groupby('label').cumcount()\n",
    "        \n",
    "    \n",
    "    # agregar una nueva columna con la suma de tiempo por cluster\n",
    "    false_orden_dfs['tiempo_total'] = false_orden_dfs.groupby('label')['tiempo'].transform('sum')\n",
    "    \n",
    "    \n",
    "    # Aplicar la función reverse_geocode a cada fila del DataFrame\n",
    "    false_orden_dfs['origen'] = false_orden_dfs.apply(lambda row: reverse_geocode(row['lat'], row['lon']), axis=1)\n",
    "    \n",
    "    # Reiniciar los labels desde cero\n",
    "    false_orden_dfs = false_orden_dfs.reset_index(drop=True)\n",
    "    false_orden_dfs['label'] = false_orden_dfs['label'].apply(lambda x: x - false_orden_dfs['label'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df['label'] = np.nan\n",
    "\n",
    "# Agrega una nueva columna con la fecha y hora de recogida combinadas\n",
    "true_df['datetime'] = true_df['fecha'] + ' ' + true_df['hora_recogida']\n",
    "\n",
    "# Agrupa por fecha y hora de recogida combinadas, y por hora de recogida\n",
    "datetime_grouped = true_df.groupby('datetime')\n",
    "\n",
    "# Recorre cada grupo y aplica clustering\n",
    "for name, group in datetime_grouped:\n",
    "    # Calcula la distancia entre cada par de direcciones\n",
    "    X = np.zeros((len(group), len(group)))\n",
    "    for i in range(len(group)):\n",
    "        address1 = group.iloc[i]['destino']\n",
    "        loc1 = geolocate(address1)  # Utilizamos nuestra función personalizada para geolocalizar la dirección\n",
    "        if loc1 is not None:\n",
    "            lat1, lon1 = loc1\n",
    "            point1 = (lat1, lon1)\n",
    "            for j in range(i+1, len(group)):\n",
    "                address2 = group.iloc[j]['destino']\n",
    "                loc2 = geolocate(address2)  # Utilizamos nuestra función personalizada para geolocalizar la dirección\n",
    "                if loc2 is not None:\n",
    "                    lat2, lon2 = loc2\n",
    "                    point2 = (lat2, lon2)\n",
    "                    X[i, j] = great_circle(point1, point2).m\n",
    "                    X[j, i] = great_circle(point1, point2).m\n",
    "    \n",
    "        # Crea una matriz con las distancias\n",
    "        kmeans_1 = KMeans(n_clusters=1)\n",
    "        kmeans_1.fit(X)\n",
    "        group['label'] = kmeans_1.labels_\n",
    "        labels_1 = np.zeros(len(group))\n",
    "        \n",
    "        # Asigna un número de cluster único a cada grupo\n",
    "        last_label_1 = true_df['label'].max()\n",
    "        if np.isnan(last_label_1):\n",
    "            cluster_label_1 = 1\n",
    "        else:\n",
    "            cluster_label_1 = last_label_1 + 1\n",
    "        for i in range(len(group)):\n",
    "            if labels_1[i] == 0:\n",
    "                labels_1[i] = cluster_label_1\n",
    "                for j in range(i+1, len(group)):\n",
    "                    if labels_1[j] == 0 and np.sum(labels_1 == cluster_label_1) < 8 and X[i, j] > 0:\n",
    "                        labels_1[j] = cluster_label_1\n",
    "                if np.sum(labels_1 == cluster_label_1) > 0:\n",
    "                    cluster_label_1 += 1\n",
    "    \n",
    "    # Asigna las etiquetas de los clusters al dataframe original\n",
    "    group['label'] = labels_1\n",
    "    true_df.loc[group.index, \"label\"] = group[\"label\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 1.2.2 (un origen-> distintos destinos ; agregar origen en la ruta)\n",
    "\n",
    "true_df=true_df[['hora_recogida','address','destino','label','id','nombre_pasajero','mail','phone','datetime_1']] #######\n",
    "true_df=true_df.sort_values(by=['label'])\n",
    "\n",
    "grouped= true_df.groupby(['address','label'])\n",
    "\n",
    "for data,group in grouped:\n",
    "    group.reset_index(drop=True,inplace=True)\n",
    "    nueva_fila = {'hora_recogida': group['hora_recogida'][0], 'address': data[0], 'destino': data[0],'label': data[1]}\n",
    "    true_df = true_df.append(nueva_fila, ignore_index=True)\n",
    "    \n",
    "true_df['aux']= true_df['address']==true_df['destino']\n",
    "\n",
    "true_df=true_df.sort_values(by=['label']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 1.2.3 (un origen-> distintos destinos ; calcular ruta más eficiente)\n",
    "\n",
    "clave_api='AIzaSyAvTzCycvOetN-NA51GNqxb80d-Ma-0Azg'\n",
    "\n",
    "# Agregar columnas para latitud y longitud\n",
    "true_df['latitud'] = None\n",
    "true_df['longitud'] = None\n",
    "\n",
    "# Iterar por cada dirección y obtener las coordenadas\n",
    "for index, row in true_df.iterrows():\n",
    "    direccion = row['destino']\n",
    "    lat,lng = geocode(direccion)  #********************************************************************\n",
    "    true_df.at[index, 'latitud'] = lat\n",
    "    true_df.at[index, 'longitud'] = lng\n",
    "    \n",
    "\n",
    "# Crear una lista vacía para almacenar los dataframes con los órdenes de las ubicaciones\n",
    "orden_dfs = []\n",
    "\n",
    "\n",
    "# Agrupar el dataframe por cluster\n",
    "grupos = true_df.groupby('label')\n",
    "\n",
    "# Iterar por cada cluster\n",
    "\n",
    "for grupo, data in grupos:\n",
    "    data = data.reset_index(drop=True)\n",
    "    ubicaciones = list(zip(data['latitud'], data['longitud']))\n",
    "    \n",
    "    # Definir el primer destino como el origen\n",
    "    primer_destino_idx = data.loc[data['aux'] == True].iloc[-1].name\n",
    "    primer_destino_id = data.loc[data['aux'] == True, 'id'].iloc[-1]\n",
    "    ruta_optima = [(primer_destino_idx, primer_destino_id)]\n",
    "\n",
    "    # Iterar hasta que se hayan agregado todos los destinos a la ruta\n",
    "    while len(ruta_optima) < len(data):\n",
    "        # Obtener la ubicación actual y las ubicaciones restantes\n",
    "        ubicacion_actual_idx, ubicacion_actual_id = ruta_optima[-1]\n",
    "        ubicacion_actual = ubicaciones[ubicacion_actual_idx]\n",
    "        ubicaciones_restantes = [(i, id_) for i, id_ in zip(range(len(ubicaciones)), data['id']) if i not in [idx for idx, _ in ruta_optima]]\n",
    "        # Calcular la distancia de la ubicación actual a cada ubicación restante       #********************************************************************\n",
    "        distances = gmaps.distance_matrix(ubicacion_actual, [ubicaciones[i] for i, _ in ubicaciones_restantes], mode='driving')\n",
    "        distances = distances['rows'][0]['elements']\n",
    "        # Ordenar las ubicaciones restantes por distancia al siguiente destino\n",
    "        sorted_indices = sorted(range(len(distances)), key=lambda k: distances[k]['distance']['value'])\n",
    "        \n",
    "        # Elegir el siguiente destino como la ubicación más cercana\n",
    "        siguiente_destino_idx, siguiente_destino_id = ubicaciones_restantes[sorted_indices[0]]\n",
    "        # Agregar el siguiente destino y su id a la ruta óptima\n",
    "        ruta_optima.append((siguiente_destino_idx, siguiente_destino_id))\n",
    "        \n",
    "    # Crear un DataFrame y añadir el orden y el id de data\n",
    "    orden_ubicaciones = [(ubicaciones[i], id_) for i, id_ in ruta_optima]\n",
    "    # Divide cada tupla en dos columnas separadas\n",
    "    coordenadas=[tupla[0] for tupla in orden_ubicaciones]\n",
    "    ids=[tupla[1] for tupla in orden_ubicaciones]\n",
    "    df_ids = pd.DataFrame(ids, columns=['id'])\n",
    "    orden_df = pd.DataFrame(coordenadas, columns=['lat', 'lon'])\n",
    "    orden_df = pd.concat([orden_df, df_ids], axis=1)\n",
    "    orden_df['orden'] = range(len(orden_df))  \n",
    "    orden_df['label'] = grupo\n",
    "    \n",
    "    # Agregar el dataframe con el orden de las ubicaciones a la lista de dataframes\n",
    "    orden_dfs.append(orden_df)\n",
    "\n",
    "if len(orden_dfs)!=0:\n",
    "    # Concatenar los dataframes en un solo dataframe\n",
    "    true_orden_dfs = pd.concat(orden_dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#paso 1.2.4 (un origen-> distintos destinos ; obtener duración de ruta)\n",
    "\n",
    "if 'true_orden_dfs' in globals():\n",
    "    duraciones = []\n",
    "    last_label = 0  # Variable para llevar la cuenta del último label utilizado\n",
    "    print(last_label)\n",
    "    for data, group in true_orden_dfs.groupby('label'):\n",
    "        durations = []\n",
    "        durations.append(None)\n",
    "        for i in range(len(group)-1):\n",
    "            route = directions(origin=group.iloc[i],   #***********************************************************************\n",
    "                                    destination=group.iloc[i+1],\n",
    "                                    waypoints=group.iloc[i+1:-1].values.tolist())                   ########### asignar hora de partida, según el grupo\n",
    "            duration = route[0]['legs'][0]['duration']['value']\n",
    "            durations.append(duration)\n",
    "        # Agregar la lista de las durations de cada recorrido a la lista duraciones\n",
    "        duraciones += durations\n",
    "            \n",
    "    #crear dataframe con las duraciones y concatenarlo al dataframe con los ordenes\n",
    "    duraciones=pd.DataFrame(duraciones,columns=['tiempo'])\n",
    "    duraciones['tiempo']=duraciones['tiempo'].apply(lambda x: round(x/60,1))\n",
    "\n",
    "    true_orden_dfs=pd.concat([true_orden_dfs,duraciones],axis=1)\n",
    "\n",
    "    # rellenar los valores NaN con ceros\n",
    "    true_orden_dfs['tiempo'] = true_orden_dfs['tiempo'].fillna(0)\n",
    "    \n",
    "    # agregar una nueva columna con la suma de tiempo por cluster\n",
    "    true_orden_dfs['tiempo_total'] = true_orden_dfs.groupby('label')['tiempo'].transform('sum')\n",
    "    \n",
    "    # Dividir el grupo si el tiempo total es mayor a 90\n",
    "    if true_orden_dfs['tiempo_total'].max() > 90:\n",
    "        group_size = len(group)\n",
    "        num_subgroups = 2\n",
    "        \n",
    "        # Calcular el número de elementos por subgrupo\n",
    "        elements_per_subgroup = group_size // num_subgroups\n",
    "        remainder = group_size % num_subgroups\n",
    "        \n",
    "        # Verificar si es necesario ajustar el tamaño de los subgrupos\n",
    "        if remainder == 0:\n",
    "            subgroups = np.array_split(group, num_subgroups)\n",
    "        else:\n",
    "            subgroups = []\n",
    "            start = 0\n",
    "            for i in range(num_subgroups):\n",
    "                end = start + elements_per_subgroup + (1 if i < remainder else 0)\n",
    "                subgroup = group[start:end]\n",
    "                subgroups.append(subgroup)\n",
    "                start = end\n",
    "        \n",
    "        # Calcular las duraciones de los subgrupos\n",
    "        duraciones = []\n",
    "        for g in subgroups:\n",
    "            durations = []\n",
    "            durations.append(None)\n",
    "            for i in range(len(g)-1):\n",
    "                route = directions(origin=g.iloc[i], #***********************************************************************\n",
    "                                        destination=g.iloc[i+1],\n",
    "                                        waypoints=g.iloc[i+1:-1].values.tolist())                   ########### asignar hora de partida, según el grupo\n",
    "                duration = route[0]['legs'][0]['duration']['value']\n",
    "                durations.append(duration)\n",
    "            # Agregar la lista de las durations de cada recorrido a la lista duraciones\n",
    "            duraciones += durations\n",
    "        duraciones=pd.DataFrame(duraciones,columns=['tiempo'])\n",
    "        duraciones['tiempo']=duraciones['tiempo'].apply(lambda x: round(x/60,1))\n",
    "        \n",
    "        # Concatenar los resultados para cada subgrupo\n",
    "        subgroups_dataframes = []\n",
    "        last_label = true_orden_dfs['label'].max()\n",
    "        for i, subgroup in enumerate(subgroups):\n",
    "            # Incrementar el label para el nuevo subgrupo\n",
    "            last_label += 1\n",
    "            subgroup_dataframe = pd.DataFrame(subgroup).assign(label=last_label, tiempo=duraciones[i:])\n",
    "            subgroups_dataframes.append(subgroup_dataframe)\n",
    "        \n",
    "        true_orden_dfs = pd.concat(subgroups_dataframes)\n",
    "        # Reiniciar la columna \"orden\" para cada nuevo label creado\n",
    "        true_orden_dfs['orden'] = true_orden_dfs.groupby('label').cumcount()\n",
    "        \n",
    "    # agregar una nueva columna con la suma de tiempo por cluster\n",
    "    true_orden_dfs['tiempo_total'] = true_orden_dfs.groupby('label')['tiempo'].transform('sum')\n",
    "    \n",
    "    #Aplicar la función reverse_geocode a cada fila del DataFrame\n",
    "    true_orden_dfs['destino'] = true_orden_dfs.apply(lambda row: reverse_geocode(row['lat'], row['lon']), axis=1)\n",
    "    # Reiniciar los labels desde cero\n",
    "    true_orden_dfs = true_orden_dfs.reset_index(drop=True)\n",
    "    true_orden_dfs['label'] = true_orden_dfs['label'].apply(lambda x: x - true_orden_dfs['label'].min())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paso 2 concatenar ambos dfs (true_orden_dfs y false_orden_dfs)\n",
    "\n",
    "\n",
    "\n",
    "unordered_df=pd.concat([false_df,true_df])\n",
    "\n",
    "if 'false_orden_dfs' in globals():\n",
    "    false_orden_dfs.dropna(subset='id',inplace=True)\n",
    "    test_false=pd.merge(false_orden_dfs,unordered_df,on='id',how='left',suffixes=('_x', '_y'),validate='one_to_many')\n",
    "\n",
    "if 'true_orden_dfs' in globals():\n",
    "    true_orden_dfs.dropna(subset='id',inplace=True)\n",
    "    test_true=pd.merge(true_orden_dfs,unordered_df,on='id',how='left',suffixes=('_x', '_y'),validate='one_to_many')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'test_false' in globals():\n",
    "    test_false['address']=test_false['origen']\n",
    "    test_false.drop(columns=['origen'],inplace=True)\n",
    "    test_false.drop_duplicates(subset=['label_x','address','tiempo','destino'],inplace=True,keep='last')\n",
    "\n",
    "if 'test_true' in globals() and 'test_false' in globals():\n",
    "    test_true['destino']=test_true['destino_x']\n",
    "    test_true.drop(columns=['destino_x'],inplace=True)\n",
    "    test_true['label_x']=test_true['label_x']+len(test_false['label_x'].unique())+1\n",
    "    test_true.drop_duplicates(subset=['label_x','address','tiempo','destino'],inplace=True,keep='last')\n",
    "    test_true['aux']= test_true['address']==test_true['destino']\n",
    "    test_true=test_true[test_true['aux']!=True]\n",
    "    test_true=test_true[['label_x','orden','hora_recogida','address','destino','tiempo','id','nombre_pasajero','mail','phone','datetime_1','tiempo_total']]\n",
    "    \n",
    "elif 'test_true' in globals() and 'test_false' not in globals():\n",
    "    test_true['destino']=test_true['destino_x']\n",
    "    test_true.drop(columns=['destino_x'],inplace=True)\n",
    "    test_true.drop_duplicates(subset=['label_x','address','tiempo','destino'],inplace=True,keep='last')\n",
    "    test_true['aux']= test_true['address']==test_true['destino']\n",
    "    test_true=test_true[test_true['aux']!=True]\n",
    "    test_true=test_true[['label_x','orden','hora_recogida','address','destino','tiempo','id','nombre_pasajero','mail','phone','datetime_1','tiempo_total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'test_true' in globals() and 'test_false' not in globals():\n",
    "    df = test_true\n",
    "elif 'test_true' not in globals() and 'test_false' in globals():\n",
    "    df = test_false\n",
    "elif 'test_true' in globals() and 'test_false' in globals():\n",
    "    df=pd.concat([test_false,test_true])\n",
    "    df=df[['label_x','mail','nombre_pasajero','address','destino','datetime_1','hora_recogida','phone','tiempo','orden','tiempo_total']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_x</th>\n",
       "      <th>mail</th>\n",
       "      <th>nombre_pasajero</th>\n",
       "      <th>address</th>\n",
       "      <th>destino</th>\n",
       "      <th>datetime_1</th>\n",
       "      <th>hora_recogida</th>\n",
       "      <th>phone</th>\n",
       "      <th>tiempo</th>\n",
       "      <th>orden</th>\n",
       "      <th>tiempo_total</th>\n",
       "      <th>Ruta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>jvalde38@jci.com</td>\n",
       "      <td>VALDEBENITO JIMENEZ, JARLYN NAZARETH</td>\n",
       "      <td>Ramaditas 1626, 2351765 Valparaíso, Chile</td>\n",
       "      <td>8 Nte. 1168, Viña del Mar, Valparaíso, Chile</td>\n",
       "      <td>2023-03-09 06:00:00</td>\n",
       "      <td>6:00</td>\n",
       "      <td>926050049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>jtorr273@jci.com</td>\n",
       "      <td>TORRES TORRES, MARIA PATRICIA</td>\n",
       "      <td>Hnos Pinzón 24, 2391336 Valparaíso, Chile</td>\n",
       "      <td>8 Nte. 1168, Viña del Mar, Valparaíso, Chile</td>\n",
       "      <td>2023-03-09 06:00:00</td>\n",
       "      <td>6:00</td>\n",
       "      <td>957105246</td>\n",
       "      <td>18.5</td>\n",
       "      <td>1</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>jrolliv@jci.com</td>\n",
       "      <td>ROLLINO VARGAS, VALENTINA GIGLIOLA</td>\n",
       "      <td>Parinacota 354, Viña del Mar, Valparaíso, Chile</td>\n",
       "      <td>8 Nte. 1168, Viña del Mar, Valparaíso, Chile</td>\n",
       "      <td>2023-03-09 06:05:00</td>\n",
       "      <td>6:05</td>\n",
       "      <td>979785561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>jdomin54@jci.com</td>\n",
       "      <td>DOMINGUEZ PEGUERO, MIRIAM ESPERANZA</td>\n",
       "      <td>Viana 1155, Valparaíso, Viña del Mar, Valparaí...</td>\n",
       "      <td>8 Nte. 1168, Viña del Mar, Valparaíso, Chile</td>\n",
       "      <td>2023-03-09 06:05:00</td>\n",
       "      <td>6:05</td>\n",
       "      <td>936334969</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>jvaccam@jci.com</td>\n",
       "      <td>VACCARO LÓPEZ, MARÍA JOSÉ</td>\n",
       "      <td>3 Ote. 1270, local 3, Viña del Mar, Valparaíso...</td>\n",
       "      <td>8 Nte. 1168, Viña del Mar, Valparaíso, Chile</td>\n",
       "      <td>2023-03-09 06:05:00</td>\n",
       "      <td>6:05</td>\n",
       "      <td>965637198</td>\n",
       "      <td>11.1</td>\n",
       "      <td>2</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_x              mail                       nombre_pasajero  \\\n",
       "0      0.0  jvalde38@jci.com  VALDEBENITO JIMENEZ, JARLYN NAZARETH   \n",
       "1      0.0  jtorr273@jci.com         TORRES TORRES, MARIA PATRICIA   \n",
       "2      1.0   jrolliv@jci.com    ROLLINO VARGAS, VALENTINA GIGLIOLA   \n",
       "3      1.0  jdomin54@jci.com   DOMINGUEZ PEGUERO, MIRIAM ESPERANZA   \n",
       "4      1.0   jvaccam@jci.com             VACCARO LÓPEZ, MARÍA JOSÉ   \n",
       "\n",
       "                                             address  \\\n",
       "0          Ramaditas 1626, 2351765 Valparaíso, Chile   \n",
       "1          Hnos Pinzón 24, 2391336 Valparaíso, Chile   \n",
       "2    Parinacota 354, Viña del Mar, Valparaíso, Chile   \n",
       "3  Viana 1155, Valparaíso, Viña del Mar, Valparaí...   \n",
       "4  3 Ote. 1270, local 3, Viña del Mar, Valparaíso...   \n",
       "\n",
       "                                        destino          datetime_1  \\\n",
       "0  8 Nte. 1168, Viña del Mar, Valparaíso, Chile 2023-03-09 06:00:00   \n",
       "1  8 Nte. 1168, Viña del Mar, Valparaíso, Chile 2023-03-09 06:00:00   \n",
       "2  8 Nte. 1168, Viña del Mar, Valparaíso, Chile 2023-03-09 06:05:00   \n",
       "3  8 Nte. 1168, Viña del Mar, Valparaíso, Chile 2023-03-09 06:05:00   \n",
       "4  8 Nte. 1168, Viña del Mar, Valparaíso, Chile 2023-03-09 06:05:00   \n",
       "\n",
       "  hora_recogida      phone  tiempo  orden  tiempo_total  Ruta  \n",
       "0          6:00  926050049     0.0      0          35.7   0.0  \n",
       "1          6:00  957105246    18.5      1          35.7   0.0  \n",
       "2          6:05  979785561     0.0      0          32.6   1.0  \n",
       "3          6:05  936334969    17.7      1          32.6   1.0  \n",
       "4          6:05  965637198    11.1      2          32.6   1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#minor improvements\n",
    "df['aux']= df['address']==df['destino']\n",
    "df['Ruta']=df['label_x']\n",
    "df=df[df['aux']!=True]\n",
    "df.drop(columns=['aux'],inplace=True)\n",
    "df['phone'] = df['phone'].apply(lambda x: round(x))\n",
    "\n",
    "#dividir en 2 cuando es > 90 minutos\n",
    "\n",
    "df.sort_values(by=['datetime_1','label_x']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traer los\n",
    "#same_origin.sort_values(by=['address','destino'])\n",
    "\n",
    "#same_origin['cluster'] = same_origin.groupby(['address','destino']).ngroup()+1\n",
    "\n",
    "#same_origin.sort_values(by=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel('reservas-adt-3.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final.sort_values(by=['label','orden']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución del segundo bloque de código: 31.56 segundos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "end_time = time.time()\n",
    "print(\"Tiempo de ejecución del segundo bloque de código: %.2f segundos\" % (end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7224ece411c9dd322214926371e93f477dfdf89a3f2a86d87350b103306a521f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
